{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c718a5c1",
   "metadata": {},
   "source": [
    "# Feature Engineering for text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c516f7e",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca4fdbd",
   "metadata": {},
   "source": [
    "Oftentimes textual data contains a lot of noise and redundancies which can potentially decrease the performance of a machine learning model trained on these. Luckily there are quite a few approaches to filter out a lot of the unwanted text and leave us with the most significant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790f52ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd561b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Hello there, this Is an Example sentence containing 9 words!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c755866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    return re.sub(\"[.,!?:;-='...\\\"@#_]\", \"\", text)\n",
    "\n",
    "s_clean = remove_punctuation(s)\n",
    "\n",
    "print(s)\n",
    "print(s_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca40d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(text):\n",
    "    return re.sub(\"\\\\d+\", \"\", text)\n",
    "\n",
    "s_clean = remove_numbers(s_clean)\n",
    "\n",
    "print(s)\n",
    "print(s_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143d7bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cacdfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text,stopwords):\n",
    "    s_stop = [w for w in text.split() if w not in stop_words]\n",
    "    return \" \".join(s_stop)\n",
    "\n",
    "def remove_stopwords_lower(text,stopwords):\n",
    "    s_stop = [w for w in text.lower().split() if w not in stop_words]\n",
    "    return \" \".join(s_stop)\n",
    "\n",
    "s_clean = remove_stopwords(s_clean,stop_words)\n",
    "print(s)\n",
    "print(s_clean)\n",
    "\n",
    "s_clean = remove_stopwords_lower(s_clean,stop_words)\n",
    "print(s_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b38229",
   "metadata": {},
   "source": [
    "### Stemming / Text normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd5d273",
   "metadata": {},
   "source": [
    "Textual data contains a lot of different variations of the same word, so when we are counting the occurences we are not getting the right results. To map the counts of all these slight variations to the same base word, different techniques can be applied. This whole process is called ___stemming___ as we try to reduce each word down to its word stem form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd4a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "stemmer.stem(\"currencies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a9ee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a function that uses stemming to bring down each word in a given text to its base form and then return the edited text.\n",
    "# Use this function to further clean up the example text.\n",
    "\n",
    "def stem_words_in_text(text,stemmer):\n",
    "    stemmed_text = [stemmer.stem(w) for w in text.split()]\n",
    "    return \" \".join(stemmed_text)\n",
    "\n",
    "s_clean_stemmer = stem_words_in_text(s_clean,stemmer)\n",
    "\n",
    "print(s)\n",
    "print(s_clean_stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d77f0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize(\"currencies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f22b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a function that uses lemmatization to bring down each word in a given text to its base form and then return the edited text.\n",
    "# Use this function to further clean up the example text.\n",
    "\n",
    "def lemmatize_words_in_text(text,lemmatizer):\n",
    "    lemmatized_text = [lemmatizer.lemmatize(w) for w in text.split()]\n",
    "    return \" \".join(lemmatized_text)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "s_clean_lemmatizer = lemmatize_words_in_text(s_clean,lemmatizer)\n",
    "print(s_clean_lemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a170d76",
   "metadata": {},
   "source": [
    "## Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18254fa",
   "metadata": {},
   "source": [
    "In order to classify or analyze textual data, we first need to transform it into a numerical representation. One of the simplest approaches to do this is called ___Bag-of-Words___. All words and their respective counts in the dataset are used to create the feature vector for our machine learning model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab4569e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fce455a",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a662b7a",
   "metadata": {},
   "source": [
    "Your task in this exercise is to take this \"deepnlp\" dataset and use it to perform classification of individual texts. Take a look at the provided data and transform the dataframe in a suitable way, the aim is to correctly predict the values in the \"class\" column.\n",
    "Preprocess the texts first, then create the Bag-of-Words that will be used in training and testing of the classification model (logistic regression). Evaluate the score of the model on the test dataset and create a confusion matrix from the results. You should furthermore use this matrix to calculate metrics like precision, recall and accuracy and plot these in a Precision-Recall-Curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfbf87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"deepnlp/Sheet_1.csv\")\n",
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20c3465",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"class\",\"response_text\"])\n",
    "\n",
    "df[\"class\"] = df[\"class\"].apply(lambda x: 1 if x==\"not_flagged\" else -1)\n",
    "df[\"class\"].plot(kind=\"hist\")\n",
    "plt.title(\"Values of \\\"class\\\" column\")\n",
    "plt.show()\n",
    "\n",
    "df[\"response_text\"] = df[\"response_text\"].apply(remove_punctuation)\n",
    "df[\"response_text\"] = df[\"response_text\"].apply(remove_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f295b1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X1 = df[[\"response_text\"]]\n",
    "y = df[[\"class\"]]\n",
    "\n",
    "X1_train, X1_test, y_train, y_test = train_test_split(X1,y,test_size=0.25, random_state=2564)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81487b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(token_pattern=\"(?u)\\\\b\\\\w+\\\\b\")\n",
    "\n",
    "train_bows1 = vectorizer.fit_transform(X1_train[\"response_text\"])\n",
    "test_bows1 = vectorizer.transform(X1_test[\"response_text\"])\n",
    "t1 = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"len(t1): {}\".format(len(t1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479ed092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/34165731/a-column-vector-y-was-passed-when-a-1d-array-was-expected\n",
    "def evaluate_score(X_train, X_test, y_train, y_test):\n",
    "    model = linear_model.LogisticRegression().fit(X_train, y_train.values.ravel())\n",
    "    score = model.score(X_test, y_test.values.ravel())\n",
    "    return (model, score)\n",
    "\n",
    "(model1, r1) = evaluate_score(train_bows1,test_bows1,y_train,y_test)\n",
    "\n",
    "print(\"Score without stopwords removed: %0.3f\" % r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6820b943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_recall_curve\n",
    "\n",
    "preds_model1 = model1.predict(test_bows1)\n",
    "\n",
    "conf_matrix1 = confusion_matrix(preds_model1, y_test)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix1)\n",
    "disp.plot()\n",
    "plt.title(\"Confusion matrix for model1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6f12a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TN = conf_matrix1[0][0]\n",
    "# FN = conf_matrix1[1][0]\n",
    "# TP = conf_matrix1[1][1]\n",
    "# FP = conf_matrix1[0][1]\n",
    "def calc_precision(conf_matrix):\n",
    "    return conf_matrix[1][1]/(conf_matrix[1][1]+conf_matrix[0][1])\n",
    "    \n",
    "def calc_recall(conf_matrix):\n",
    "    return conf_matrix[1][1]/(conf_matrix[1][1]+conf_matrix[1][0])\n",
    "\n",
    "def calc_accuracy(conf_matrix):\n",
    "    return (conf_matrix[1][1]+conf_matrix[0][0])/(conf_matrix[0][0]+conf_matrix[0][1]+conf_matrix[1][0]+conf_matrix[1][1])\n",
    "\n",
    "def calc_metrics(conf_matrix):\n",
    "    return calc_precision(conf_matrix), calc_recall(conf_matrix), calc_accuracy(conf_matrix)\n",
    "\n",
    "prec1,rec1,acc1 = calc_metrics(conf_matrix1)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(preds_model1,y_test)\n",
    "plt.plot(recall,precision,label=\"model1\")\n",
    "\n",
    "plt.title(\"Precision-Recall curve\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47de29ac",
   "metadata": {},
   "source": [
    "#### Extension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd2874b",
   "metadata": {},
   "source": [
    "Now extend your work by applying different transformations, as shown above, as well as different n_grams to the dataset. Your task is to cover all the combinations listed below and find the best option for this classification task. Try to use lists, functions and loops to automatically generate the results as there are 18 different models to be considered:\n",
    "<br>\n",
    "\n",
    "df:\n",
    " 1. untouched \n",
    "     1. 1-gram\n",
    "     1. 2-gram\n",
    "     1. 3-gram        \n",
    " 1. stem\n",
    "     1. 1-gram\n",
    "     1. 2-gram\n",
    "     1. 3-gram\n",
    " 1. lemma \n",
    "     1. 1-gram\n",
    "     1. 2-gram\n",
    "     1. 3-gram\n",
    "\n",
    "df_stopwords:\n",
    " 1. untouched \n",
    "     1. 1-gram\n",
    "     1. 2-gram\n",
    "     1. 3-gram        \n",
    " 1. stem\n",
    "     1. 1-gram\n",
    "     1. 2-gram\n",
    "     1. 3-gram\n",
    " 1. lemma \n",
    "     1. 1-gram\n",
    "     1. 2-gram\n",
    "     1. 3-gram\n",
    "    \n",
    "<br>\n",
    "Tips: \n",
    "\n",
    "- df.copy()\n",
    "- x1_train, x1_test, x2_train, x2_test ... = train_test_split(X1,X2,...,Xx,y,test_size=0.25, random_state=2564) (you can individually split an arbitrary amount of input data)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6171b887",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"deepnlp/Sheet_1.csv\")\n",
    "print(df.info())\n",
    "df.head()\n",
    "\n",
    "df = df.dropna(subset=[\"class\",\"response_text\"])\n",
    "\n",
    "df[\"class\"] = df[\"class\"].apply(lambda x: 1 if x==\"not_flagged\" else -1)\n",
    "df[\"class\"].plot(kind=\"hist\")\n",
    "plt.title(\"Values of \\\"class\\\" column\")\n",
    "plt.show()\n",
    "\n",
    "df[\"response_text\"] = df[\"response_text\"].apply(remove_punctuation)\n",
    "df[\"response_text\"] = df[\"response_text\"].apply(remove_numbers)\n",
    "\n",
    "df_stem = df.copy()\n",
    "df_stem[\"response_text\"] = df_stem[\"response_text\"].apply(stem_words_in_text,stemmer=stemmer)\n",
    "\n",
    "df_lemma = df.copy()\n",
    "df_lemma[\"response_text\"] = df_lemma[\"response_text\"].apply(lemmatize_words_in_text,lemmatizer=lemmatizer)\n",
    "\n",
    "df_stopwords = df.copy()\n",
    "df_stopwords[\"response_text\"] = df_stopwords[\"response_text\"].apply(remove_stopwords_lower,stopwords=stop_words)\n",
    "\n",
    "df_stopwords_stem = df_stopwords.copy()\n",
    "df_stopwords_stem[\"response_text\"] = df_stopwords_stem[\"response_text\"].apply(stem_words_in_text,stemmer=stemmer)\n",
    "\n",
    "df_stopwords_lemma = df_stopwords.copy()\n",
    "df_stopwords_lemma[\"response_text\"] = df_stopwords_lemma[\"response_text\"].apply(lemmatize_words_in_text,lemmatizer=lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6e7d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = df[[\"response_text\"]]\n",
    "X2 = df_stem[[\"response_text\"]]\n",
    "X3 = df_lemma[[\"response_text\"]]\n",
    "X4 = df_stopwords[[\"response_text\"]]\n",
    "X5 = df_stopwords_stem[[\"response_text\"]]\n",
    "X6 = df_stopwords_lemma[[\"response_text\"]]\n",
    "\n",
    "y = df[[\"class\"]]\n",
    "\n",
    "\n",
    "X1_train, X1_test, X2_train, X2_test, X3_train, X3_test, X4_train, X4_test, X5_train, X5_test, X6_train, X6_test, y_train, y_test = train_test_split(X1,X2,X3,X4,X5,X6,y,test_size=0.25, random_state=2564)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d068bb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bows(train_df,test_df,column_name,vectorizer):\n",
    "    return [vectorizer.fit_transform(train_df[column_name]),vectorizer.transform(test_df[column_name])]\n",
    "\n",
    "all_dfs = [[X1_train,X1_test],[X2_train,X2_test],[X3_train,X3_test],[X4_train,X4_test],[X5_train,X5_test],[X6_train,X6_test]]\n",
    "all_data = []\n",
    "all_preds = []\n",
    "all_cms = []\n",
    "all_models = []\n",
    "all_prcs = []\n",
    "\n",
    "vectorizer = CountVectorizer(token_pattern=\"(?u)\\\\b\\\\w+\\\\b\")\n",
    "vectorizer_bigram = CountVectorizer(ngram_range=(2,2),token_pattern=\"(?u)\\\\b\\\\w+\\\\b\")\n",
    "vectorizer_trigram = CountVectorizer(ngram_range=(3,3),token_pattern=\"(?u)\\\\b\\\\w+\\\\b\")\n",
    "\n",
    "for dfs in all_dfs:\n",
    "    all_data.append(create_bows(dfs[0],dfs[1],\"response_text\",vectorizer))\n",
    "    all_data.append(create_bows(dfs[0],dfs[1],\"response_text\",vectorizer_bigram))\n",
    "    all_data.append(create_bows(dfs[0],dfs[1],\"response_text\",vectorizer_trigram))\n",
    "    \n",
    "for data in all_data:\n",
    "    all_models.append(evaluate_score(data[0],data[1],y_train,y_test))\n",
    "\n",
    "for i,model in enumerate(all_models):\n",
    "    print(\"Score for model{}: {:0.3f}\".format(i+1,model[1]))\n",
    "    \n",
    "    preds = model[0].predict(all_data[i][1])\n",
    "    all_preds.append(preds)\n",
    "    \n",
    "    conf_matrix = confusion_matrix(preds, y_test)\n",
    "    all_cms.append(conf_matrix)\n",
    "    \n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n",
    "    disp.plot()\n",
    "    plt.title(\"Confusion matrix for model{}\".format(i+1))\n",
    "    plt.show()\n",
    "    \n",
    "    precision, recall, thresholds = precision_recall_curve(preds,y_test)\n",
    "    all_prcs.append([precision,recall,thresholds])\n",
    "    \n",
    "\n",
    "for i,prc in enumerate(all_prcs):\n",
    "    plt.plot(prc[1],prc[0],label=\"model{}\".format(i+1))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
